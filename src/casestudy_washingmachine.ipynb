{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stumpy \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "parent = os.path.dirname(current)\n",
    "sys.path.append(parent)\n",
    "\n",
    "from MSig import Motif, NullModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "colnames = [\"timestamp\",\"current\",\"power\",\"power_factor\",\"water_monitoring_inlet\",\n",
    "            \"water_monitoring_outlet\",\"water_temperature_inlet\",\"water_temperature_outlet\",\n",
    "            \"water_temperature_drum_door_temp\",\"water_temperature_drum_temp\",\n",
    "            \"ambient_temperature\",\"humidity\",\"ambient_temperature_t2\",\"humidity_t2\",\"water_pressure\"]\n",
    "data = pd.read_csv(\"../data/washingmachine/main_readings.csv\", usecols=colnames)\n",
    "#data_accellerometer = pd.read_csv(\"../data/washingmachine/main_readings_accelerometer.csv\")\n",
    "#change timestamp from float to datetime\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'], unit='s')\n",
    "#drop the microsecond part\n",
    "data['timestamp'] = data['timestamp'].dt.floor('s')\n",
    "#remove readings in same second\n",
    "data = data.drop_duplicates(subset='timestamp', keep='first')\n",
    "data = data.set_index('timestamp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregate accelerometer readings by second\n",
    "#change date to timestamp\n",
    "# data_accellerometer['timestamp'] = pd.to_datetime(data_accellerometer['date'])\n",
    "# data_accellerometer = data_accellerometer.set_index('timestamp')\n",
    "# data_accellerometer = data_accellerometer.resample('s').mean()\n",
    "# data_accellerometer = data_accellerometer.reset_index()\n",
    "# data_accellerometer[\"timestamp\"] = pd.to_datetime(data_accellerometer[\"timestamp\"])\n",
    "# data_accellerometer['timestamp'] = data_accellerometer['timestamp'].dt.floor('s')\n",
    "# data_accellerometer['timestamp'] = data_accellerometer['timestamp'].dt.tz_localize(None)\n",
    "# data_accellerometer = data_accellerometer.dropna()\n",
    "\n",
    "#join the two dataframes\n",
    "\n",
    "#data_accellerometer = data_accellerometer.set_index('timestamp')\n",
    "#data = data.join(data_accellerometer, how='inner')\n",
    "#data = data.drop(columns=['voltage', \"frequency\", \"power_factor\"])\n",
    "#features = data.columns.tolist()\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to numpy ndarray\n",
    "X = data.to_numpy().astype(np.float64).T\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include = None\n",
    "normalize = True\n",
    "subsequence_lengths = [30, 60, 300, 600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in subsequence_lengths:\n",
    "    mp, mp_indices = stumpy.mstump(X, m, include=include, normalize=normalize)\n",
    "    np.save('../results/washingmachine/mp/mp_normalized={}_include={}_m={}.npy'.format(normalize,include,m), mp)\n",
    "    np.save('../results/washingmachine/mp_indices/mp_indices_normalized={}_include={}_m={}.npy'.format(normalize,include,m), mp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_summary_motifs(motif_indices, motif_distances, motif_subspaces, data, m, normalize, max_allowed_dist, excl_zone):\n",
    "    mp_stats_table = pd.DataFrame(columns=[\"ID\", \"k\", \"Features\", \"m\", \"#Matches\", \"Indices\", \"max(dists)\", \"min(dists)\", \"med(dists)\"])\n",
    "\n",
    "    motif_index = 0\n",
    "\n",
    "    n_vars, n_time = data.shape\n",
    "\n",
    "    if normalize:\n",
    "        data = (data - np.mean(data, axis=1)[:, np.newaxis]) / np.std(data, axis=1)[:, np.newaxis]\n",
    "    \n",
    "    model_empirical = NullModel(data, model=\"empirical\")\n",
    "    model_gaussian_empirical = NullModel(data, model=\"kde\")\n",
    "    model_gaussian_theoretical = NullModel(data, model=\"gaussian_theoretical\")\n",
    "\n",
    "    for motif_indice, match_indices in enumerate(motif_indices):\n",
    "\n",
    "        dimensions = motif_subspaces[motif_indice]\n",
    "            \n",
    "        #remove filling values of -1 and Nans from motif_indices and match_distances\n",
    "        match_indices = match_indices[match_indices != -1]\n",
    "        match_distances = motif_distances[motif_indice]\n",
    "        match_distances = match_distances[~np.isnan(match_distances)]\n",
    "\n",
    "        #if is empty, skip\n",
    "        if len(match_indices) == 0:\n",
    "            continue\n",
    "        \n",
    "        #remove trivial matches  \n",
    "        non_trivial_matches = []\n",
    "        for indice in match_indices:\n",
    "           trivial = False\n",
    "           for indice_new in non_trivial_matches:\n",
    "               if abs(indice - indice_new) < m/4:\n",
    "                   trivial = True\n",
    "                   break\n",
    "           if not trivial:\n",
    "               non_trivial_matches.append(indice)\n",
    "        match_indices = non_trivial_matches\n",
    "\n",
    "        #get the multidim time serie motif in the dimensions\n",
    "        multivar_subsequence = data[dimensions][:,match_indices[0]:match_indices[0]+m]\n",
    "    \n",
    "        max_dist = np.max(match_distances)\n",
    "        min_dist = np.min(match_distances[1:])\n",
    "        avg_dist = np.mean(match_distances[1:])\n",
    "        std_dist = np.std(match_distances[1:])\n",
    "        med_dist = np.median(match_distances[1:])\n",
    "        \n",
    "        #D is distance profile between the motif pattern and Time serie\n",
    "        if max_allowed_dist is None:\n",
    "            D = np.empty((n_time-m+1, len(dimensions)))\n",
    "            for i, dimension in enumerate(dimensions):\n",
    "                D[:,i] = stumpy.mass(multivar_subsequence[i], data[dimension], normalize=normalize)\n",
    "            D = np.mean(D, axis=1)\n",
    "            max_allowed_dist = np.nanmax([np.nanmean(D) - 2.0 * np.nanstd(D), np.nanmin(D)])\n",
    "            \n",
    "\n",
    "        #data features are now the ones in the dimensions\n",
    "        used_features = [f\"{dimension}\" for dimension in dimensions]\n",
    "\n",
    "        # dist_avg_elemento = SQRT((dist^2)/n_length)\n",
    "        #max_delta = max_allowed_dist  # dist_max motif = dist_max element (worst case) max_dist = sqrt(max_delta^2) <=> max_delta = max_dist\n",
    "        max_delta = math.sqrt(max_allowed_dist**2/m) \n",
    "        delta_thresholds = [max_delta]*len(data)\n",
    "\n",
    "        \n",
    "        #########SIG#########\n",
    "        motif = Motif(multivar_subsequence, dimensions, delta_thresholds, len(match_indices))\n",
    "        p = motif.set_pattern_probability(model_empirical, vars_indep=True)\n",
    "        pvalue = motif.set_significance(n_time-excl_zone, n_vars, idd_correction=False) \n",
    "        ######\n",
    "        #p = motif.set_pattern_probability(model_gaussian_empirical, vars_indep=True)\n",
    "        #pvalue = motif.set_significance(n_time-excl_zone, n_vars, idd_correction=False)\n",
    "        ######\n",
    "        #p = motif.set_pattern_probability(model_gaussian_theoretical, vars_indep=True)\n",
    "        #pvalue = motif.set_significance(n_time-excl_zone, n_vars, idd_correction=False)\n",
    "\n",
    "        stats_df = {\"ID\": str(motif_index), \"k\":len(dimensions),\n",
    "                    \"Features\":\",\".join(used_features),\n",
    "                        \"m\":m,\n",
    "                    \"#Matches\": len(match_indices)-1,\n",
    "                        \"Indices\":match_indices,\n",
    "                        \"max(dists)\": np.around(max_dist,3), \"min(dists)\": np.around(min_dist,3),\n",
    "                        \"med(dists)\": np.around(med_dist,3),  \"P\": p, \"p-value\": pvalue}\n",
    "    \n",
    "        mp_stats_table = pd.concat(\n",
    "            [mp_stats_table, pd.DataFrame.from_records([stats_df])], ignore_index=True)\n",
    "        \n",
    "        motif_index += 1\n",
    "    return mp_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = None\n",
    "min_neighbors = 1\n",
    "max_distance =  None\n",
    "cutoffs = np.inf\n",
    "max_matches = 99999\n",
    "max_motifs = 99999\n",
    "mp_stats_table = pd.DataFrame()\n",
    "for m in subsequence_lengths:\n",
    "    \n",
    "    excl_zone = int(np.ceil(m/4))\n",
    "\n",
    "    mp= np.load('../results/washingmachine/mp/mp_normalized={}_include={}_m={}.npy'.format(normalize,include,m))\n",
    "    mp_indices = np.load('../results/washingmachine/mp_indices/mp_indices_normalized={}_include={}_m={}.npy'.format(normalize,include,m))\n",
    "    motif_distances, motif_indices, motif_subspaces, motif_mdls = stumpy.mmotifs(X, mp, mp_indices, max_distance=max_distance,max_matches=max_matches,\n",
    "                                                                                 cutoffs=cutoffs, min_neighbors=min_neighbors, max_motifs=max_motifs, k=k, include=include, normalize=normalize)\n",
    "\n",
    "    print(\"m:{}, #Motifs:{}\".format(m, len(motif_indices)))  \n",
    "    table = table_summary_motifs(motif_indices, motif_distances, motif_subspaces, X, m, normalize, max_distance, excl_zone)\n",
    "    print(\"Sig \", np.sum(table[\"p-value\"] < 0.01))\n",
    "\n",
    "    #hochberg procedure\n",
    "    p_values = table[\"p-value\"].to_numpy()\n",
    "    critical_value =  NullModel.hochberg_critical_value(p_values, 0.05)\n",
    "    sig = table[\"p-value\"] < critical_value\n",
    "    table[\"Sig_Hochber\"] = sig\n",
    "\n",
    "    print(\"Sig after Hochberg: {}, critical value: {}\".format(np.sum(sig), critical_value))\n",
    "\n",
    "\n",
    "    mp_stats_table = pd.concat([mp_stats_table, table], ignore_index=True)\n",
    "\n",
    "mp_stats_table.to_csv('../results/washingmachine/table_motifs_min_neighbors={}_max_distance={}_cutoffs={}_max_matches={}_max_motifs={}.csv'.format(min_neighbors, max_distance, cutoffs, max_matches, max_motifs), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new table for each motif length with statistics of the motifs (number of motifs found,\n",
    "# number of significant motifs, average number of matches +- std, average of features +- std, \n",
    "#average probability +- std, average pvalue +- std)\n",
    "mp_stats_table = pd.read_csv('../results/washingmachine/table_motifs_min_neighbors={}_max_distance={}_cutoffs={}_max_matches={}_max_motifs={}.csv'.format(min_neighbors, max_distance, cutoffs, max_matches, max_motifs))\n",
    "\n",
    "motif_lengths = mp_stats_table[\"m\"].unique()\n",
    "motif_stats_table = pd.DataFrame(columns=[\"m\", \"#motifs\" , \"avg_n_matches\", \"avg_n_features\",  \"avg_probability\",  \"avg_pvalue\", \"#sig_motifs(<0.01)\", \"#sig_hochberg\"])\n",
    "for m in motif_lengths:\n",
    "    table = mp_stats_table[mp_stats_table[\"m\"] == m]\n",
    "    n_motifs = table.shape[0]\n",
    "    n_sig_motifs_001 = table[table[\"p-value\"] < 0.01].shape[0]\n",
    "    n_sig_motifs_hochberg = table[table[\"Sig_Hochber\"] == True].shape[0]\n",
    "    avg_n_matches = round(table[\"#Matches\"].mean(),2), round(table[\"#Matches\"].std(),3)\n",
    "    avg_n_features = round(table[\"k\"].mean(),2), round(table[\"k\"].std(),3)\n",
    "    avg_probability = table[\"P\"].mean(), table[\"P\"].std()    \n",
    "    avg_pvalue = table[\"p-value\"].mean(), table[\"p-value\"].std()\n",
    "\n",
    "    stats_df = {\"m\": m, \"#motifs\": n_motifs, \"#sig_motifs(<0.01)\": n_sig_motifs_001, \"#sig_hochberg\": n_sig_motifs_hochberg,\n",
    "                \"avg_n_matches\": avg_n_matches, \"avg_n_features\": avg_n_features, \"avg_probability\": avg_probability, \"avg_pvalue\": avg_pvalue}\n",
    "    motif_stats_table = pd.concat([motif_stats_table, pd.DataFrame.from_records([stats_df])], ignore_index=True)\n",
    "\n",
    "motif_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_stats_table_print = motif_stats_table.copy()\n",
    "motif_stats_table_print[\"avg_n_matches\"] = motif_stats_table[\"avg_n_matches\"].apply(lambda x: \"{:.2f} +- {:.2f}\".format(x[0], x[1]))\n",
    "motif_stats_table_print[\"avg_n_features\"] = motif_stats_table[\"avg_n_features\"].apply(lambda x: \"{:.2f} +- {:.2f}\".format(x[0], x[1]))\n",
    "motif_stats_table_print[\"avg_probability\"] = motif_stats_table[\"avg_probability\"].apply(lambda x: \"{:.2e} +- {:.2e}\".format(x[0], x[1]))\n",
    "motif_stats_table_print[\"avg_pvalue\"] = motif_stats_table[\"avg_pvalue\"].apply(lambda x: \"{:.2e} +- {:.2e}\".format(x[0], x[1]))\n",
    "print(motif_stats_table_print.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort by score unified\n",
    "mp_stats_table = mp_stats_table.sort_values(by=\"p-value\", ascending=True)\n",
    "mp_stats_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 3 most significant for each motif length\n",
    "for m in motif_lengths:\n",
    "    top_3_motifs = mp_stats_table[mp_stats_table[\"m\"] == m].head(3)\n",
    "    print(top_3_motifs.to_latex(index=False, columns=[\"ID\", \"k\", \"Features\", \"#Matches\", \"max(dists)\", \"min(dists)\", \"med(dists)\", \"P\", \"p-value\"]))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_motif(ts_list, features,  m, motif_indexes, motif_name):\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=len(ts_list), figsize=(10, 3*len(ts_list)), squeeze=False)\n",
    "    for i in range(0,len(ts_list)):\n",
    "        ts = ts_list[i]\n",
    "        #plot light grey\n",
    "        axes[i,1].plot(ts, color='black', linewidth=0.5, alpha=0.5)\n",
    "\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(motif_indexes)))\n",
    "        axes[i,0].set_prop_cycle('color', colors)\n",
    "        axes[i,1].set_prop_cycle('color', colors)\n",
    "\n",
    "        for index in motif_indexes:\n",
    "            subsequence_match = ts.iloc[index:index+m]\n",
    "            #original motif in the next plot with the same color\n",
    "            axes[i,0].plot(subsequence_match.values) \n",
    "            # highlight the motif in the original time serie\n",
    "            axes[i,1].plot(subsequence_match, linewidth=2)\n",
    "        \n",
    "        plt.setp(axes[i,0].xaxis.get_majorticklabels(), rotation=90)\n",
    "        #remove x labels and ticks except from last plot\n",
    "        if i != len(ts_list)-1:\n",
    "            axes[i,0].axes.get_xaxis().set_visible(False)\n",
    "            axes[i,1].axes.get_xaxis().set_visible(False)\n",
    "\n",
    "        plt.setp(axes[i,0].xaxis.get_majorticklabels(), rotation=90)\n",
    "\n",
    "\n",
    "        #format the x axis to show the time and rotate for better reading\n",
    "        axes[i,1].xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45)\n",
    "        axes[i,0].set_ylabel(features[i], rotation=90, size='large')\n",
    "\n",
    "\n",
    "    #title of the fig\n",
    "    axes[0,0].set_title(\"Raw Subsequences\")\n",
    "    axes[0,1].set_title(\"Motif in TS\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../results/audio/motif_'+str(motif_name)+\".pdf\",bbox_inches='tight')\n",
    "  \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot top motif\n",
    "for m in subsequence_lengths:\n",
    "    print(\"Motif length: \", m)\n",
    "    top_motifs = mp_stats_table[mp_stats_table[\"m\"] == m].sort_values(by=\"p-value\").head(3)\n",
    "    for top_motif in top_motifs.to_dict(orient=\"records\"): \n",
    "        m = top_motif[\"m\"]\n",
    "        dimensions = top_motif[\"Features\"].split(\",\")\n",
    "        dimensions = [int(dimension) for dimension in dimensions]\n",
    "        features = [data.columns[dimension] for dimension in dimensions]\n",
    "        indices = top_motif['Indices'].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "        indices = [int(i) for i in indices]\n",
    "        motif_name = top_motif[\"ID\"]\n",
    "        ts_list = [data[feature] for feature in features]\n",
    "        plot_motif(ts_list, features, m, indices, motif_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
